# Теория вероятности

## Событие и вероятность

### Случайный эксперимент

**Эксперимент** – наблюдение явления в заданных условиях.

**Исход** – результат эксперимента.

![](/pics/sample-space.png)

$\Omega$ – **пространство исходов** это множество всех исходов, описывающее все возможные варианты того, что может случиться в результате эксперимента.
$\omega_1, \omega_2, ..., \omega_n$ – элементы $\Omega$, где $n$ – количество исходов эксперимента.

$\omega \in \Omega$ – читается как исход $\omega$ из пространства исходов $\Omega$.

![](/pics/event.png)

**Событие** – набор исходов случайного эксперимента, удовлетворяющий определённым условиям. Или, по-другому, подмножество $\Omega$.

$A \subset \Omega$ – читается как «событие $A$ – подмножество пространства исходов $\Omega$», возможна также и запись через условие:
$A = \{ \text{число очков на кубике чётное} \}= \{ A \in \Omega : \omega \vdots 2 \} = \{ 2, 4, 6 \}$

Все те исходы $\omega$, принадлежащие событию $A$ (то есть $\omega \in A$), называют **благоприятными исходами** для события $A$.

$\omega \not \in A$ – **неблагоприятные исходы**.

### Вероятность

#### Типы событий

$A = \varnothing$ – **невозможное событие** это то, которое заведомо не произойдёт в результате эксперимента.

$A = \Omega$ – **достоверное событие** это то, которое обязательно произойдёт.

**Случайные события** это все оставшиеся события (не невозможные и не достоверные).

#### Вероятность события

**Относительная частота события** $A$ – это отношение числа случаев $n$, в которых событие появилось к общему числу случаев $m$: $$W(A)=\frac{n}{m}$$
![](/pics/probability.png)

$P(A) = \frac {\lvert A \rvert}{\lvert \Omega \rvert}$ – **вероятность $P$ события $A$** есть соотношение мощностей множеств $A$ и $\Omega$, при условии что все исходы в эксперименте равновероятны. Для вычисления вероятности необходимо разделить число благоприятных событию исходов на число всех исходов.

#### Вероятность в общем случае

$P(\Omega)=1$ – **сумма вероятностей** всех исходов равна $1$.

$P(A) = \sum_{\omega \in A} P(\omega)$ – вычисление **вероятности в общем случае**.

#### Геометрическое определение вероятности

$$P(A) = \frac{\text{геометрический размер фигуры, благоприятной для A}}{\text {геометрический размер всей фигуры}}$$

## Взаимодействие событий

### Противоположные события

$\overline A$ – **событие противоположно** событию $A$, если оно состоит из тех исходов $\Omega$, которых нет в $A$, иначе говоря это дополнение множества $A$ до $\Omega$, $\overline A$ читается как «не $A$».

$\overline A = \Omega \setminus A = \{ x: x \in \Omega \land x \notin A \}$

$A \cup \overline A = \Omega$

$A \cap \overline A = \varnothing$



$P(A) = 1 - P(\overline A)$

Теорема о вероятности наступления *хотя бы одного* из независимых событий: 
вероятность появления хотя бы одного из независимых событий равна разности между единицей и произведением вероятной противоположных событий – $P(A) = 1 - q_1 \cdot q_2 \cdot ... \cdot q_n$,
если вероятности независимых событий равны ($q_1 = q_2 = ... = q_n$), то $P(A) = 1 - q^n$

### Несовместные события

$A_i \cap A_j = \varnothing$ , для всех $i \not = j$ – **несовместные события** $A_i$ и $A_j$ не пересекаются.

**Полная группа событий** – набор несовместных событий, которые в объединении дают всё пространство исходов $\Omega$. 

![](/pics/event-types.png)

### Совместные события и сложение вероятностей

$A \cap B \neq \varnothing$ – **совместные события $A$ и $B$** 

![](/pics/joint-events.png)

**Вероятность объединения *несовместных* событий** равна сумме вероятностей этих событий:
$P(A \lor B) = P(A) + P(B)$

**Вероятность объединения *совместных* событий** равна сумме вероятностей этих событий минус вероятность их пересечения:
$P(A \cup B) = P(A) + P(B) − P(A \cap B)$ 

### Условная вероятность

$P(B \lvert A) = \frac{P(A \cap B)}{P(A)}$ – **вероятность события $B$ при условии $A$**

Вероятность того что произойдёт событие $B$ при условии $A$, и вероятность того, что произойдут оба события $A$ и $B$, – разные значения. Они различаются в $P(A)$ раз.

#### Произведение вероятностей

$P(A \cap B) = P(A) \cdot P(B \lvert A)$

Событие $A$ не зависит от $B$ если $P(A \lvert B) = P(A)$

#### Условная вероятность

Свойства условной вероятности:
$\frac{P(B \lvert A)}{P(A \lvert B)} = \frac{P(B)}{P(A)}$ – значит, $P(B) > P(A)$ равносильно $P(B \lvert A) > P(A \lvert B)$

Если $A \subset B$, то $P(B \lvert A) = 1$

Если $B \subset A$, то $P(B \lvert A) = \frac{P(B)}{P(A)}$

Если $B_1 \cap B_2 = \varnothing$ , то $P(B_1 \cup B_2 \lvert A) = P(B_1 \lvert A) + P(B_2 \lvert A)$

$P(B \lvert A) + P(\overline{B} \lvert A) = 1$ , но $P(B \lvert A) + P(B \lvert \overline{A}) \not = 1$ в общем случае.

### Теорема Байеса

**Априорная вероятность** – *безусловная* вероятность случайного события, назначаемая до наблюдаемых событий, которые могут на неё повлиять.

**Апостериорная вероятность** – *условная* вероятность случайного события, назначаемая после наблюдения событий, которые могут на неё повлиять. Условие здесь: произошедшее событие.

**Теорема Байеса** – для любых A и B верно равенство:
$P(A \lvert B) = \frac{P(B \lvert A)}{P(B)} ⋅ P(A)$

**Формула полной вероятности**:
$P(A) = \sum_{i=1}^n P(A \lvert B_i) ⋅ P(B_i)$ – если $B_1, B_2, ..., B_n$ – полная группа событий.

$P(A) = P(A \lvert B) ⋅ P(B) + P(A \lvert \overline B) ⋅ P(\overline B)$ – для $B$ и $\overline B$, всегда образующих полную группу событий.

## Случайные величины
### Случайная величина и её распределение

#### Случайная величина

**Случайная величина** – численная характеристика случайного эксперимента. Её значение заранее неизвестно и зависит от свершившегося исхода.



Новая случайная величина $Z$, для которой выполнено: 

$Z(w) = X(w) + Y(w)$ для каждого $w \in \Omega$, 

называется **суммой случайных величин** $X$ и $Y$.



Новая случайная величина $Z$, для которой выполнено: 

$Z(w) = X(w) \cdot Y(w)$ для каждого $w \in \Omega$, 

называется **произведением случайных величин** $X$ и $Y$.



Новая случайная величина $Z$, для которой выполнено: 

$Z(w) = X(w)^p (p \in \Bbb N)$ для каждого $w \in \Omega$, 

называется **степенью $p$ случайной величины** $X$.

#### Распределение случайной величины

Связь $p(x) = P(X = x)$ между значением случайной величины и его вероятностью называют **распределением случайной величины $X$**.

Для любой случайной величины $X$ верно, что: 
$P(X = x) = \sum_{X(w) = x} P(w)$

#### Распределение Бернулли

Если случайная величина $X$ принимает только два значения 1 («успех») и 0 («неуспех») и вероятность успеха $p \in [0, 1]$, то говорят что X имеет **распределение Бернулли**. 
Обозначается как $X \sim Bern(p)$.

#### Равномерное распределение

Если случайная величина $X$ принимает целые значения в интервале от $a$ до $b$ и $P(X = x) = \frac{1}{n}$ для любого $x \in [a, b]$, то говорят что $x$ имеет **равномерное распределение**. 
$n = b - a + 1$ – количество значений, которые принимает $X$.
Обозначается как $X \sim U([a, b])$.

### Характеристики случайной величины

#### Математическое ожидание

Если случайная величина $X$ принимает значения $x_1, x_2, ..., x_n$, то её **математическое ожидание** (*expectation* или *среднее значение*) определяется как: 
$E(X) = \sum_{i=1}^n x_i \cdot P(X = x_i)$

##### Свойства математического ожидания

$E(aX) = aE(X)$ – для *любой* случайной величины $X$ и действительного числа – среднее значение случайной величины меняется во столько раз, во сколько изменяется её значение.

$E(X+Y) = E(X) + E(Y)$ – для *любых* случайных $X$ и $Y$.

$E(X \cdot Y) = E(X) \cdot E(Y)$ – для *независимых* $X$ и $Y$.

Если событие $X$ является *достоверным*, т.е. наступающим с вероятностью 1, то $P(X = x) = 1$ при любых исходах. Такие случайные величины называются **постоянными**. 

Если $X$ является постоянной $a$, то $E(X)=a$.

#### Дисперсия

Если случайная величина $X$ принимает значения $x_1, x_2, ..., x_n$, то её **дисперсия** (*variance*) определяется как:
$Var(X) = \sum_{i=1}^n(x_i - E(X))^2 ⋅ P(X = x_i) = E ((X - E(X))^2)$

$Var(X) = E(X^2) - (E(X))^2$

$Var(C) = 0$ – дисперсия постоянной величины $C$ равна нулю.
$Var(CX) = C^2Var(X)$ – постоянную величину (константу) можно вынести за знак дисперсии, возведя её в квадрат.

$Var(X \pm Y) = Var(X) + Var(Y)$ , где $X$ и $Y$ *независимые* случайные величины.

$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X, Y)$ – для *любых* случайных $X$ и $Y$.

#### Взаимосвязь случайных величин

**Ковариацией** двух случайных величин $X$ и $Y$ называют: 
$Cov(X, Y) = E((X-E(X))(Y-E(Y))) = E(X ⋅ Y) - E(X) ⋅ E(Y)$

$Cov(X, Y) = E((X - E(X))(Y - E(Y))) = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_x) \cdot (y_j - \mu_y) \cdot P(X = x_i \cap Y = y_j)$

$Cov(X, Y) = E(X \cdot Y) - E(X) \cdot E(Y) = \sum_{i=1}^n \sum_{j=1}^m x_i \cdot y_j \cdot P(X = x_i \cap Y = y_j) - \mu_x \cdot \mu_y$

Ковариация может принимать как положительные, так и отрицательные значения. Она будет положительной, если с ростом одной случайной величины значения второй имеют тенденцию возрастать. И отрицательной, если убывать.